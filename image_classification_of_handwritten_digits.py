# -*- coding: utf-8 -*-
"""Image Classification of Handwritten Digits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-lYKmFhsFxq-zKPsdgufvbsW0Oq_eAj9

# Computer Vision: Image Classification of Handwritten Digits
"""

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import datasets, transforms,models
from sklearn.decomposition import PCA
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import random_split, DataLoader
from scipy.ndimage import gaussian_filter
import torch.nn.functional as F
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import confusion_matrix, classification_report
from torchvision.models import resnet18, ResNet18_Weights

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

"""### Define transformations for the dataset"""

transform = transforms.Compose([
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to range [-1, 1]
])

"""# Load the dataset

## Dataset Description
The MNIST Handwritten Digits dataset is a classic benchmark in machine learning, widely used for image classification tasks.

It contains 70,000 grayscale images of handwritten digits from 0 to 9, split into 60,000 training and 10,000 test images.

Each image is a 28x28 pixel grid, flattened into a 784-dimensional vector, with labels indicating the digit each image represents.

### Classification Task
The objective is to classify each image into one of ten classes (digits 0-9) using a machine learning model.

This involves training a model to recognize patterns in the pixel data and accurately predict the digit in new images.
"""

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

"""# 1. Data Exploration

### Create data loaders for batching
"""

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

"""### Check dataset size"""

print(f'Training set size: {len(train_dataset)} images')
print(f'Test set size: {len(test_dataset)} images')

"""# 1.2 Visualize Random Images

## Function to display random images

### Display random images to observe handwriting styles and data variety
"""

def display_random_images(data_loader, num_images=5):
    data_iter = iter(data_loader)
    images, labels = next(data_iter)

    fig, axes = plt.subplots(1, num_images, figsize=(10, 2))
    for i in range(num_images):
        img = images[i].numpy().squeeze()  # Remove extra dimensions
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'Label: {labels[i].item()}')
        axes[i].axis('off')
    plt.show()

display_random_images(train_loader)

"""## Explanation of Key Sections:
### Function Definition:

**dataset:** Expects an iterable dataset, ideally one where each element is a tuple of (image, label).
**num_images:** The number of images to display; defaults to 5.

**Figure Setup:**

plt.figure(figsize=(10, 2)) initializes the plot with a horizontal aspect ratio suitable for a row of images.

**Loop and Subplot Creation:**

The loop iterates up to num_images times, displaying each image and its label.
plt.subplot(1, num_images, i + 1) ensures each image is in its own subplot within a single row.
Image Display and Error Handling:

plt.imshow(image.numpy().squeeze(), cmap='gray') converts images (e.g., PyTorch Tensors) to NumPy arrays if needed.

If any image fails to load, the error message identifies the problematic index.
This function is adaptable for visualizing samples from image datasets like MNIST, where images and labels are stored together in tuples.

## Inspect dataset structure and size
"""

print("Dataset Structure:")
print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of test samples: {len(test_dataset)}")

"""#### Check the shape of one batch of images and labels"""

data_iter = iter(train_loader)
images, labels = next(data_iter)
print(f"Batch size: {images.shape[0]}")
print(f"Image dimensions: {images.shape[2]}x{images.shape[3]}")
print(f"Label batch shape: {labels.shape}")

"""#### Display random images from the first batch"""

def display_random_images(images, labels, num_images=5):
    plt.figure(figsize=(10, 2))
    for i in range(num_images):
        image = images[i].numpy().squeeze()  # Remove channel dimension for grayscale
        label = labels[i].item()

        plt.subplot(1, num_images, i + 1)
        plt.imshow(image, cmap='gray')
        plt.title(f"Label: {label}")
        plt.axis('off')

    plt.show()

# Display a few images from the batch
display_random_images(images, labels, num_images=5)

"""#### Check label distribution in the training set"""

all_labels = [label for _, label in train_loader]
all_labels = torch.cat(all_labels).numpy()
unique_labels, counts = np.unique(all_labels, return_counts=True)

print("\nLabel Distribution in Training Set:")
for label, count in zip(unique_labels, counts):
    print(f"Label {label}: {count} instances")

"""1. **Dataset Inspection**:
   - `len(train_dataset)` and `len(test_dataset)` print the number of samples in each dataset.
   - The shape of a batch is displayed by loading one batch of `images` and `labels` using `next(data_iter)`. The shape helps confirm batch size and image dimensions.

2. **Visualize Sample Images**:
   - `display_random_images` displays a few random images from the first batch to observe different handwriting styles and digit shapes. Images are converted to NumPy arrays and displayed using Matplotlib.

3. **Label Distribution**:
   - Using `np.unique`, unique labels and counts are computed for the entire training set, providing insight into class balance.

This setup provides a clear understanding of the dataset structure, image dimensions, and label distribution, key steps for effective model training and analysis.

## View Image Properties:
Confirm image dimensions and format (28x28 pixels, grayscale) and check for unexpected values.
"""

# Get one batch of images and labels
data_iter = iter(train_loader)
images, labels = next(data_iter)

# Confirm image dimensions and format
print("Image Properties:")
print(f"Batch shape: {images.shape}")
print(f"Single image shape: {images[0].shape}")
print(f"Image data type: {images.dtype}")
print(f"Min pixel value: {images.min().item()}")
print(f"Max pixel value: {images.max().item()}")

# Visualize the first image in the batch to confirm dimensions and grayscale format
plt.imshow(images[0].squeeze(), cmap='gray')
plt.title(f"Label: {labels[0].item()}")
plt.axis('off')
plt.show()

# Verify that all images in the batch are 28x28 grayscale
all_28x28 = all(image.shape == (1, 28, 28) for image in images)
if all_28x28:
    print("All images are confirmed to be 28x28 pixels in grayscale format.")
else:
    print("Warning: Not all images are 28x28 pixels in grayscale format.")

"""1. **Get a Batch and Inspect Properties**:
   - Using `next(data_iter)`, we retrieve one batch of images and labels.
   - Print statements confirm:
     - Batch shape, showing the batch size, channel count (should be 1 for grayscale), and image dimensions.
     - A single image shape for verification.
     - Data type of images (should be `torch.float32` due to `ToTensor()` transformation).
     - Minimum and maximum pixel values, expected to be between 0.0 and 1.0 after transformation.

2. **Visualize the First Image**:
   - `plt.imshow()` visualizes the first image in grayscale to confirm its format visually.

3. **Validate Dimensions**:
   - `all(image.shape == (1, 28, 28) for image in images)` verifies that all images in the batch are 28x28 pixels and grayscale.

## Class Distribution Analysis:
Plot a histogram to check the distribution of each digit (0-9) to confirm balance.

1. **Extract Labels**:
   - `train_dataset.targets.numpy()` extracts the labels (digits) from the training dataset as a NumPy array for analysis.

2. **Plot Histogram**:
   - A histogram is plotted using `plt.hist()`, with `bins=np.arange(10) - 0.5` ensuring that the bars are centered on the integer digit values.
   - The histogram displays the frequency of each digit (0-9) to visually confirm the balance.

3. **Add Titles and Labels**:
   - Titles and axis labels are added for clarity, along with grid lines for better readability.

4. **Count and Print Instances**:
   - Using `np.unique(labels, return_counts=True)`, the code counts the instances of each digit and prints these counts for further inspection.

This analysis will provide insight into the balance of the dataset, which is crucial for ensuring effective model training.
"""

# Analyze class distribution in the training set
labels = train_dataset.targets.numpy()  # Extract labels as a NumPy array

# Plotting the histogram for class distribution
plt.figure(figsize=(10, 5))
plt.hist(labels, bins=np.arange(10) - 0.5, edgecolor='black', align='mid')
plt.title('Class Distribution of Digits (0-9) in MNIST Dataset')
plt.xlabel('Digits')
plt.ylabel('Frequency')
plt.xticks(np.arange(10))  # Set x-ticks for each digit
plt.grid(axis='y', linestyle='--')
plt.show()

# Print counts of each class for further inspection
unique, counts = np.unique(labels, return_counts=True)
for digit, count in zip(unique, counts):
    print(f"Digit {digit}: {count} instances")

"""## Image Intensity Analysis:
Calculate basic statistics (mean, standard deviation) of pixel intensities to understand brightness and contrast across images.

1. **Function for Intensity Statistics**:
   - `calculate_intensity_statistics(dataset)` iterates over all images in the provided dataset.
   - For each image:
     - The image tensor is converted to a NumPy array and flattened to a 1D array for easier calculations.
     - It calculates the total number of pixels and accumulates the sum of pixel intensities and the sum of squared pixel intensities.

2. **Mean and Standard Deviation Calculation**:
   - The mean intensity is calculated by dividing the total sum of intensities by the number of pixels.
   - Variance is calculated from the sum of squared intensities, and the standard deviation is derived from the variance.

3. **Display Results**:
   - Finally, the mean and standard deviation of pixel intensities are printed to the output.

This analysis provides insights into the overall brightness and contrast of the MNIST images, which is useful for understanding the dataset's characteristics and guiding preprocessing steps for model training.
"""

# Function to calculate mean and standard deviation of pixel intensities
def calculate_intensity_statistics(dataset):
    total_pixels = 0
    sum_intensity = 0.0
    sum_intensity_squared = 0.0

    # Loop through all images in the dataset
    for image, _ in dataset:
        # Convert the image tensor to numpy array and flatten it
        image = image.numpy().flatten()
        total_pixels += image.size
        sum_intensity += np.sum(image)
        sum_intensity_squared += np.sum(image ** 2)

    mean_intensity = sum_intensity / total_pixels
    variance = (sum_intensity_squared / total_pixels) - (mean_intensity ** 2)
    std_dev_intensity = np.sqrt(variance)

    return mean_intensity, std_dev_intensity

# Calculate and display statistics for the training dataset
mean_intensity, std_dev_intensity = calculate_intensity_statistics(train_loader)

print(f"Mean Pixel Intensity: {mean_intensity:.4f}")
print(f"Standard Deviation of Pixel Intensity: {std_dev_intensity:.4f}")

"""### Noise and Quality Check:
Manually inspect a few random images to identify any unclear or noisy digits.

1. **Function to Show Random Images**:
   - `show_random_images(dataset, num_images=5)` randomly selects a specified number of images from the dataset.
   - It generates a figure with subplots to display these images.
   - Each image is converted to a NumPy array and displayed using `plt.imshow()` with a grayscale color map.
   - The title of each subplot shows the label corresponding to the digit.

2. **Random Sampling**:
   - The function uses `random.sample()` to select unique random indices from the dataset, ensuring a variety of digits and styles in the output.

3. **Display Images**:
   - Finally, the function is called to display five random images from the training dataset for manual inspection.

By running this code, you will be able to visually inspect several random images from the MNIST dataset, allowing you to identify any unclear or noisy digits that may affect model performance.
"""

# Function to display a few random images from the dataset
import random
# Function to display a few random images from the dataset
def show_random_images(dataset, num_images=5):
    plt.figure(figsize=(10, 2))
    indices = random.sample(range(len(dataset)), num_images)  # Randomly sample indices
    for i, index in enumerate(indices):
        image, label = dataset[index]
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image.numpy().squeeze(), cmap='gray')
        plt.title(f"Label: {label}")  # Remove .item() if label is an int
        plt.axis('off')
    plt.show()

# Display random images to inspect for noise and quality
show_random_images(train_dataset, num_images=5)

"""### Shape and Format Consistency Check:
Verify that all images have the expected 28x28 shape and consistent formatting.

1. **Define Expected Shape**:
   - The correct shape `(1, 28, 28)` is set, where `1` refers to the single grayscale channel, and `28x28` is the pixel resolution.

2. **Function for Shape Consistency Check**:
   - `check_image_shape_and_format(dataset)` iterates through each image in the dataset and verifies its shape.
   - If an image’s shape does not match the expected shape, its index and actual shape are stored in a list `inconsistent_shapes`.

3. **Output Results**:
   - If all images are consistent, a message is printed to confirm this.
   - Otherwise, the indices and shapes of inconsistent images are displayed.

4. **Run Check**:
   - The function is then applied to `train_dataset` for verification.

This code will help ensure that each image in the MNIST dataset has a consistent shape and format, which is essential for training a machine learning model without errors.
"""

# Check for shape and format consistency
def check_image_shape_and_format(dataset):
    correct_shape = (1, 28, 28)  # Expected shape for MNIST images
    consistent_format = True
    inconsistent_shapes = []

    for i in range(len(dataset)):
        image, _ = dataset[i]
        if image.shape != correct_shape:
            inconsistent_shapes.append((i, image.shape))
            consistent_format = False

    if consistent_format:
        print("All images have the expected 28x28 shape and grayscale format.")
    else:
        print("Some images have inconsistent shapes:")
        for index, shape in inconsistent_shapes:
            print(f"Image {index} has shape {shape}")

# Run the check on the training dataset
check_image_shape_and_format(train_dataset)

"""# 2. Preprocessing

Prepare the images for model training by ensuring uniformity and enhancing data quality.

### 2.1 Normalization:
Scale pixel values from 0–255 to a range of 0–1 for stability during model training.

1. **Define Transformation**:
   - `transforms.ToTensor()` converts images to PyTorch tensors, automatically normalizing pixel values from a range of `[0, 255]` to `[0, 1]`.
   - `transforms.Normalize((0.5,), (0.5,))` optionally normalizes pixel values to have a mean of 0.5 and a standard deviation of 0.5, further centering them around zero. This step is optional but often used in deep learning.

2. **Load Data with Normalization**:
   - The transformation is applied directly during dataset loading using `datasets.MNIST()` with the updated `transform`.

3. **Data Loaders**:
   - The data loaders (`train_loader` and `test_loader`) allow batching for efficient model training.

4. **Display Normalized Images**:
   - `show_sample_images()` function displays a few normalized images for visual inspection.
   - After transformation, the pixel values should appear slightly different but remain visible for the digit.

### Why Normalize?

Normalization improves numerical stability and reduces training time, especially in neural networks. By converting all images to a consistent range, the model can learn more efficiently, with the gradient updates and convergence being more stable.

This setup is ready for model training with a normalized MNIST dataset.
"""

# Display a few normalized images to confirm preprocessing

def show_sample_images(dataset, num_images=5):
    plt.figure(figsize=(10, 2))
    for i in range(num_images):
        image, label = dataset[i]
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image.numpy().squeeze(), cmap='gray')
        plt.title(f"Label: {label}")
        plt.axis('off')
    plt.show()

# Show sample images from the normalized train dataset
show_sample_images(train_dataset)

"""### 2.2 Reshaping:
Reshape images into a consistent format (e.g., 1D array if required) for model compatibility.

1. **Function to Reshape Image**:
   - `reshape_image(image)` reshapes any given image tensor from 28x28 to 1D (784,) without changing the dataset structure.
   - This function can be applied on-demand, making it flexible for scenarios where only some images need reshaping.

2. **Verification**:
   - The function is tested by displaying the original and reshaped image dimensions for a sample image.

This approach ensures that you have full control over when and where the reshaping is applied, keeping the original 2D structure in the dataset while preparing images as 1D arrays as required for specific model inputs.
"""

# Function to reshape an image to a 1D array
def reshape_image(image):
    return image.view(-1)  # Converts 28x28 tensor to a 784-element 1D tensor

# Test the reshaping function on a single image from the dataset
# Grab one image and label from the train dataset
image, label = train_dataset[0]

# Print original and reshaped image shapes
print("Original image shape:", image.shape)
reshaped_image = reshape_image(image)
print("Reshaped image shape:", reshaped_image.shape)

"""## 2.3 Train-Test Split:

Split the dataset into training and testing sets, typically using an 80-20 or 70-30 ratio.

1. **Splitting**:
   - We calculate `train_size` as 80% of `train_dataset` and assign the remaining 20% to `val_size`.
   - `random_split` is used to create the `train_data` and `val_data` subsets.

2. **Data Loaders**:
   - `train_loader` and `val_loader` are created for the new training and validation sets, while `test_loader` remains for the original test dataset.

3. **Verification**:
   - We print the sizes of `train_data`, `val_data`, and `test_dataset` to confirm the split.

This setup provides an 80-20 split of `train_dataset` for training and validation purposes, while maintaining `test_dataset` as the hold-out test set.
"""

# Define the split sizes for an 80-20 split on the training data
train_size = int(0.8 * len(train_dataset))  # 80% of the training data
val_size = len(train_dataset) - train_size  # Remaining 20% as validation data

# Split the training dataset into train and validation sets
train_data, val_data = random_split(train_dataset, [train_size, val_size])

# Data loaders for batching
train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(dataset=val_data, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Verify the sizes of each set
print(f"Training set size: {len(train_data)}")
print(f"Validation set size: {len(val_data)}")
print(f"Test set size: {len(test_dataset)}")

"""### 2.4 Data Augmentation:

Apply slight random transformations (e.g., rotations or zooms) to improve model generalization.

1. **Augmentation Transform**:
   - `RandomRotation(degrees=15)`: Randomly rotates images by up to ±15 degrees.
   - `RandomResizedCrop(size=28, scale=(0.9, 1.1))`: Randomly resizes and crops images to simulate zooming in or out.

2. **Loading Augmented Dataset**:
   - `train_dataset_augmented` is loaded with the augmentation transform applied, while `test_dataset` remains unaugmented to ensure the test data remains consistent.

3. **Data Loader for Augmented Dataset**:
   - `train_loader_augmented` is created with shuffling enabled for batching augmented images.
   
4. **Visualization**:
   - `show_augmented_images` function displays a few augmented images to verify the transformations, helping to visually confirm that the augmentations are applied correctly.

This setup introduces slight variations to training images, aiding the model in learning more generalized features and improving performance on unseen data.
"""

# Define data augmentation transforms for the training set
augment_transform = transforms.Compose([
    transforms.RandomRotation(degrees=15),  # Random rotation up to 15 degrees
    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),  # Random zoom in/out
    transforms.ToTensor()  # Convert to tensor
])

# Load the MNIST training set with data augmentation
train_dataset_augmented = datasets.MNIST(root='./data', train=True, download=True, transform=augment_transform)

# Load the test dataset without augmentation
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())

# Data loaders for batching
train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Visualize a few augmented images to verify transformations

def show_augmented_images(loader, num_images=5):
    plt.figure(figsize=(10, 2))
    for i, (image, label) in enumerate(loader):
        if i >= num_images:
            break
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image[0].numpy().squeeze(), cmap='gray')  # Display the first image in the batch
        plt.title(label[0].item())
        plt.axis('off')
    plt.show()

# Display a few examples of augmented images
show_augmented_images(train_loader_augmented)

"""### 2.5 Dimensionality Reduction:
Use **Principal Component Analysis (PCA)** to reduce the feature space, making training faster for some models.

Principal Component Analysis (PCA) reducesthe feature space of the MNIST dataset

1. **Flatten the Images**:
   - `flatten_data` function reshapes each 28x28 image into a 1D array of 784 pixels, which is required by PCA.

2. **Apply PCA**:
   - `PCA(n_components=0.95)` reduces the dimensionality of the data while retaining 95% of the variance.
   - `train_data_pca` and `test_data_pca` are the PCA-transformed datasets with reduced feature dimensions.

3. **Plotting Explained Variance**:
   - A cumulative variance plot shows the number of components needed to retain a certain level of variance, helping understand the impact of dimensionality reduction.

4. **Reconstructed Image**:
   - `display_reconstructed_image` reconstructs an image from the PCA-reduced data to help visualize what the reduced data represents.
   
Using PCA, we retain the most significant features of each image, leading to faster model training while preserving essential visual characteristics for classification.
"""

# Convert the dataset to flattened arrays for PCA (28x28 -> 784)
def flatten_data(dataset):
    data = []
    labels = []
    for img, label in dataset:
        img_flat = img.view(-1).numpy()  # Flatten the 28x28 image to a 784-element array
        data.append(img_flat)
        labels.append(label)
    return np.array(data), np.array(labels)

train_data, train_labels = flatten_data(train_dataset)
test_data, test_labels = flatten_data(test_dataset)

# Apply PCA to reduce dimensionality, preserving 95% of variance
pca = PCA(n_components=0.95, random_state=42)
train_data_pca = pca.fit_transform(train_data)
test_data_pca = pca.transform(test_data)

# Print the reduced shape and plot explained variance
print("Original shape:", train_data.shape)
print("Reduced shape:", train_data_pca.shape)

plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance by PCA Components')
plt.show()

# Display a PCA-transformed image by reconstructing
def display_reconstructed_image(data_pca, index):
    img_pca = pca.inverse_transform(data_pca[index])
    img = img_pca.reshape(28, 28)
    plt.imshow(img, cmap='gray')
    plt.title(f"Label: {train_labels[index]}")
    plt.axis('off')
    plt.show()

# Display one reconstructed image from PCA-transformed data
display_reconstructed_image(train_data_pca, 0)

"""### 2.6 Binarization:
Convert grayscale images to binary (black and white) to simplify the data and highlight digit shapes.

1. **Binarization Function**:
   - The `binarize_image` function takes an image tensor and a threshold value (defaulting to 0.5). It converts pixels greater than the threshold to 1 (white) and others to 0 (black), creating a binary image.

2. **Binarize Dataset**:
   - The `binarize_dataset` function iterates through the dataset, applying the binarization function to each image and storing the results.

3. **Display Binarized Images**:
   - The `show_binarized_images` function visualizes a few binarized images along with their corresponding labels to verify the binarization process.

4. **Visual Output**:
   - Finally, a few binarized images are displayed to confirm that the binarization process has successfully highlighted the shapes of the digits in the dataset.

This approach effectively simplifies the MNIST dataset, focusing on the digit shapes, which can help improve the performance of subsequent machine learning models.
"""

# Function to binarize images
def binarize_image(image, threshold=0.5):
    """Convert a grayscale image to binary based on the given threshold."""
    # Convert the image to a binary array (0 or 1)
    binary_image = (image > threshold).float()
    return binary_image

# Binarize all images in the dataset
def binarize_dataset(dataset):
    binarized_data = []
    labels = []
    for img, label in dataset:
        img_binarized = binarize_image(img.squeeze())  # Squeeze to remove channel dimension
        binarized_data.append(img_binarized.numpy())
        labels.append(label)
    return binarized_data, labels

# Binarize the training and test datasets
train_data_binarized, train_labels = binarize_dataset(train_dataset)
test_data_binarized, test_labels = binarize_dataset(test_dataset)

# Function to display binarized images
def show_binarized_images(images, labels, num_images=5):
    plt.figure(figsize=(10, 2))
    for i in range(min(num_images, len(images))):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(images[i], cmap='gray')
        plt.title(f'Label: {labels[i]}')
        plt.axis('off')
    plt.show()

# Display a few binarized images
show_binarized_images(train_data_binarized, train_labels, num_images=5)

"""### 2.7 Noise Reduction:
Apply a filter (e.g., Gaussian or median) to reduce minor noise in the images.

1. **Noise Reduction Function**:
   - The `apply_gaussian_filter` function applies a Gaussian filter to the images using `scipy.ndimage.gaussian_filter`. The `sigma` parameter controls the amount of blurring (higher values mean more blurring).

2. **Reduce Noise in Dataset**:
   - The `reduce_noise_dataset` function iterates through the dataset, applying the noise reduction function to each image and storing the filtered images and their labels.

3. **Display Comparison**:
   - The `show_images_comparison` function visualizes a few original images alongside their noise-reduced versions, allowing you to assess the effectiveness of the noise reduction.

4. **Visual Output**:
   - Finally, a few pairs of original and filtered images are displayed to confirm that the noise reduction process has effectively smoothed the images while retaining the digit shapes.

By applying noise reduction techniques, you can enhance the quality of the images in the MNIST dataset, which can help improve the performance of machine learning models during training.
"""

# Function to apply Gaussian filter for noise reduction
def apply_gaussian_filter(image, sigma=1):
    """Apply Gaussian filter to the image to reduce noise."""
    # Convert the tensor to a numpy array and apply the filter
    image_np = image.numpy()
    filtered_image = gaussian_filter(image_np, sigma=sigma)
    return filtered_image

# Function to apply noise reduction on the dataset
def reduce_noise_dataset(dataset):
    noise_reduced_data = []
    labels = []
    for img, label in dataset:
        img_filtered = apply_gaussian_filter(img.squeeze(), sigma=1)  # Squeeze to remove channel dimension
        noise_reduced_data.append(img_filtered)
        labels.append(label)
    return noise_reduced_data, labels

# Reduce noise in the training and test datasets
train_data_filtered, train_labels = reduce_noise_dataset(train_dataset)
test_data_filtered, test_labels = reduce_noise_dataset(test_dataset)

# Function to display original and filtered images side by side
def show_images_comparison(original_images, filtered_images, labels, num_images=5):
    plt.figure(figsize=(12, 4))
    for i in range(min(num_images, len(original_images))):
        # Original Image
        plt.subplot(2, num_images, i + 1)
        plt.imshow(original_images[i], cmap='gray')
        plt.title(f'Original\nLabel: {labels[i]}')
        plt.axis('off')

        # Filtered Image
        plt.subplot(2, num_images, i + 1 + num_images)
        plt.imshow(filtered_images[i], cmap='gray')
        plt.title('Filtered')
        plt.axis('off')

    plt.show()

# Display a few original and filtered images
original_images = [img.numpy().squeeze() for img, _ in train_dataset]
show_images_comparison(original_images, train_data_filtered, train_labels, num_images=5)

"""### 2.8 Standardization:
Center pixel values around a mean of 0 and scale to a standard deviation of 1, beneficial for certain models.

1. **Calculate Mean and Standard Deviation**:
   - The `calculate_mean_std` function computes the mean and standard deviation of all images in the training dataset by stacking them into a single tensor.

2. **Standardization Function**:
   - The `standardize_dataset` function applies the standardization formula to each image: \((\text{image} - \text{mean}) / \text{std}\).

3. **Standardization Application**:
   - The training and test datasets are standardized using the mean and standard deviation computed from the training dataset.

4. **Visual Output**:
   - The `show_standardized_images` function visualizes a few standardized images to confirm that they have been centered around a mean of 0 and scaled to a standard deviation of 1.

### Note:
- After standardization, the pixel values will typically range around \([-1, 1]\), depending on the original data distribution. This transformation can be beneficial for models sensitive to the scale of input data, such as neural networks.
"""

# Function to calculate mean and standard deviation of the training dataset
def calculate_mean_std(dataset):
    # Concatenate all images into a single tensor
    all_images = torch.stack([img for img, _ in dataset])
    mean = all_images.mean()
    std = all_images.std()
    return mean, std

# Calculate mean and standard deviation for the training dataset
mean, std = calculate_mean_std(train_dataset)
print(f"Mean: {mean:.4f}, Standard Deviation: {std:.4f}")

# Function to standardize the dataset
def standardize_dataset(dataset, mean, std):
    standardized_data = []
    labels = []
    for img, label in dataset:
        # Standardize the image
        img_standardized = (img - mean) / std
        standardized_data.append(img_standardized)
        labels.append(label)
    return standardized_data, labels

# Standardize the training and test datasets
train_data_standardized, train_labels = standardize_dataset(train_dataset, mean, std)
test_data_standardized, test_labels = standardize_dataset(test_dataset, mean, std)

# Function to display a few standardized images
def show_standardized_images(standardized_images, labels, num_images=5):
    plt.figure(figsize=(12, 4))
    for i in range(min(num_images, len(standardized_images))):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(standardized_images[i].numpy().squeeze(), cmap='gray')
        plt.title(f'Label: {labels[i]}')
        plt.axis('off')
    plt.show()

# Display a few standardized images
show_standardized_images(train_data_standardized, train_labels, num_images=5)

"""### 2.9 Dimensional Consistency Check:
Confirm all images are in the expected input format (28x28 or reshaped as needed for specific models).

1. **Function for Dimensional Consistency Check**:
   - The `check_dimensions` function iterates through the dataset. For each image, it checks if the shape is `(1, 28, 28)`, which indicates that the image is in the expected format (1 channel, 28 pixels height, and 28 pixels width).

2. **Output of Consistency Check**:
   - If an image does not have the expected dimensions, the function prints the index of the image and its actual shape. Finally, the function returns whether all images meet the dimensional criteria.

3. **Running the Check**:
   - The function is called for both the training and test datasets, and the results are printed to the console.

### Notes:
- This check is crucial before feeding the data into a model, as inconsistent dimensions can lead to errors during training or inference.
- In this implementation, MNIST images are assumed to be properly formatted; however, the code is robust enough to identify any deviations.
"""

# Function to check dimensional consistency
def check_dimensions(dataset):
    consistent = True
    for index, (img, label) in enumerate(dataset):
        # Check the image dimensions
        if img.shape != (1, 28, 28):  # (Channels, Height, Width)
            print(f"Image at index {index} has unexpected dimensions: {img.shape}")
            consistent = False
    return consistent

# Check dimensions for the training dataset
train_consistency = check_dimensions(train_dataset)
if train_consistency:
    print("All training images have the expected dimensions (1, 28, 28).")
else:
    print("Some training images do not have the expected dimensions.")

# Check dimensions for the test dataset
test_consistency = check_dimensions(test_dataset)
if test_consistency:
    print("All test images have the expected dimensions (1, 28, 28).")
else:
    print("Some test images do not have the expected dimensions.")

"""### One-Hot Encoding of Labels:
Transform labels into one-hot encoded vectors if the model requires categorical encoding, such as a neural network.

1. **One-Hot Encoding Function**:
   - The `one_hot_encode` function takes the labels and the number of classes (10 for MNIST) as inputs and returns the one-hot encoded representation. It uses `torch.nn.functional.one_hot` to perform the encoding, followed by `.float()` to convert the result to a floating-point tensor.

2. **Encoding Example**:
   - The code iterates through the first batch of the training data, applies one-hot encoding to the labels, and breaks out of the loop after the first batch to avoid processing all batches.

3. **Output**:
   - The original labels and their corresponding one-hot encoded versions are printed to the console for verification.

### Note:
- One-hot encoding is essential for many machine learning models, especially neural networks, as they typically expect labels in this format for multi-class classification tasks.
- This code provides a straightforward implementation that can be adjusted or expanded based on specific model requirements or data processing needs.
"""

# Function to perform one-hot encoding of labels
def one_hot_encode(labels, num_classes=10):
    return F.one_hot(labels, num_classes=num_classes).float()

# Example of encoding labels for the first batch of the training data
for images, labels in train_loader:
    one_hot_labels = one_hot_encode(labels)
    break  # Get only the first batch for demonstration

# Display the original labels and their one-hot encoded versions
print("Original Labels: ", labels)
print("One-Hot Encoded Labels:\n", one_hot_labels)

"""# 3. Model Selection and Training

**Objective:** Train a simple model to classify the images into 10 classes (digits 0-9).

1. **Preparing Data**:
   - A loop iterates through the dataset to flatten each image from a 28x28 tensor into a 784-dimensional vector and stores the labels. The flattened images and labels are then converted to numpy arrays.

2. **Train-Test Split**:
   - The dataset is split into training and testing sets using an 80-20 ratio with `train_test_split` from `scikit-learn`.

3. **Training the Logistic Regression Model**:
   - An instance of `LogisticRegression` is created and trained on the training set. The `max_iter` parameter is set to 1000 to ensure convergence.
"""

# pip install torch torchvision scikit-learn

"""### 3.1 Split the dataset into training and testing sets (80-20 ratio)"""

# Prepare data
X = []
y = []

# Convert images and labels to numpy arrays
for image, label in train_dataset:
    X.append(image.view(-1).numpy())  # Flatten the image
    y.append(label)

# Convert lists to numpy arrays
X = np.array(X)
y = np.array(y)

# Split the dataset into training and testing sets (80-20 ratio)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Traditional Models

### 3.2 Create and train the Logistic Regression model

Logistic Regression: Simple, fast, effective for small-scale tasks.
"""

logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train)

# Predict on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of Logistic Regression on MNIST: {accuracy:.4f}')

"""### 3.3 Create and train the K-Nearest Neighbors model

K-Nearest Neighbors (KNN): Classifies based on similarity to neighboring images.
"""

knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed
knn_model.fit(X_train, y_train)

# Predict on the test set
y_pred = knn_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of K-Nearest Neighbors on MNIST: {accuracy:.4f}')

"""### 3.4 Create and train the Support Vector Machine model

Support Vector Machine (SVM): Effective for high-dimensional data like images.
"""

svm_model = svm.SVC(kernel='rbf', gamma='scale')
svm_model.fit(X_train, y_train)

# Predict on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of Support Vector Machine on MNIST: {accuracy:.4f}')

"""## Deep Learning Models:

### 3.5 Simple Neural Network (Multilayer Perceptron, or MLP):
Consists of a few fully connected layers, with activation functions to introduce non-linearity.

Useful for simple image classification tasks, though performance may be limited compared to CNNs.

1. **Defining the MLP Model**:
   - The `SimpleMLP` class defines a neural network with:
     - An input layer that flattens the image (28x28).
     - Two hidden layers with ReLU activation functions.
     - An output layer with 10 outputs (for the digits 0-9).

2. **Training the Model**:
   - The model is trained using the Adam optimizer and cross-entropy loss function.
   - The training process involves forward passes, loss computation, backward propagation, and weight updates.

3. **Evaluating the Model**:
   - The accuracy is calculated by comparing predicted labels against the true labels from the test dataset.

#### 3.5.1 Define the Simple Neural Network (MLP)
"""

class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer
        self.fc2 = nn.Linear(128, 64)        # Hidden layer
        self.fc3 = nn.Linear(64, 10)         # Output layer (10 classes for digits 0-9)
        self.relu = nn.ReLU()                 # Activation function

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # Flatten the input
        x = self.relu(self.fc1(x))  # First layer with ReLU activation
        x = self.relu(self.fc2(x))  # Second layer with ReLU activation
        x = self.fc3(x)             # Output layer (no activation)
        return x

"""#### 3.5.2 Instantiate the model, define loss function and optimizer"""

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

mlp_model = SimpleMLP().to(device)
criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification
optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)  # Adam optimizer

"""#### 3.5.3 Training the model"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    mlp_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = mlp_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.5.4 Evaluating the model"""

mlp_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = mlp_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Simple MLP on the MNIST test set: {accuracy:.2f}%')

"""## 3.6 Convolutional Neural Network (CNN):
A more advanced model for image data, utilizing convolutional layers that capture spatial hierarchies in images.

Layers include convolution and pooling layers followed by one or more fully connected layers.

### Example CNN Structure:
**Convolution Layer 1:** Extracts features with filters (e.g., edge detection).

**Pooling Layer 1:** Reduces spatial dimensions, keeping essential features.

**Convolution Layer 2 + Pooling Layer 2:** Deepens feature detection (more complex shapes).

**Fully Connected Layer:** Maps the extracted features to output classes (digits 0–9).

#### 3.6.1 Define the CNN model
"""

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # Convolution Layer 1
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # 28x28x1 -> 28x28x32
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling Layer 1
        # Convolution Layer 2
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # 28x28x32 -> 28x28x64
        # Fully Connected Layer
        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 7x7x64 = 3136
        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)

    def forward(self, x):
        x = self.conv1(x)  # First convolution layer
        x = nn.ReLU()(x)  # Activation function
        x = self.pool(x)  # First pooling layer

        x = self.conv2(x)  # Second convolution layer
        x = nn.ReLU()(x)  # Activation function
        x = self.pool(x)  # Second pooling layer

        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor
        x = self.fc1(x)  # First fully connected layer
        x = nn.ReLU()(x)  # Activation function
        x = self.fc2(x)  # Output layer
        return x

"""#### 3.6.2 Instantiate the model, define loss function and optimizer"""

cnn_model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification
optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)  # Adam optimizer

"""#### 3.6.3 Training the model"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    cnn_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = cnn_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.6.4 Evaluating the model"""

cnn_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = cnn_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Simple CNN on the MNIST test set: {accuracy:.2f}%')

"""## 3.7 Pre-trained CNNs (Optional):
Use a pre-trained network like VGG16 or ResNet and fine-tune it on MNIST.

These networks come with learned features from large image datasets but can be adapted to the MNIST dataset through transfer learning, though they may be more advanced than needed for MNIST.

#### 3.7.1 Load the pre-trained ResNet model
"""

# Load a pre-trained ResNet model with the updated weights parameter
pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)
# Modify the first convolutional layer to accept 1-channel input
pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
# Modify the model to match the number of output classes
num_ftrs = pretrained_model.fc.in_features  # Get the input features of the final layer
pretrained_model.fc = nn.Linear(num_ftrs, 10)  # Replace the final layer with 10 classes

# Move the model to the appropriate device
pretrained_model = pretrained_model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(pretrained_model.parameters(), lr=0.001)

"""#### 3.7.2 Fine-tune the model"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    pretrained_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = pretrained_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.7.3 Evaluating the model"""

pretrained_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = pretrained_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Pre-trained ResNet on the MNIST test set: {accuracy:.2f}%')

"""# 4. Prediction and Evaluation

### Objective:
Test the model on unseen data and measure its performance.

#### 4.1: Make Predictions
"""

model.eval()  # Set the model to evaluation mode
y_true = []
y_pred = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels

        y_true.extend(labels.cpu().numpy())  # Append true labels
        y_pred.extend(predicted.cpu().numpy())  # Append predicted labels

"""#### 4.2: Calculate Accuracy"""

accuracy = np.mean(np.array(y_true) == np.array(y_pred)) * 100
print(f'Accuracy of the model on the MNIST test set: {accuracy:.2f}%')

"""#### 4.3: Confusion Matrix"""

cm = confusion_matrix(y_true, y_pred)

# Visualizing the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

def get_predictions(model, images):
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        if isinstance(model, torch.nn.Module):  # For PyTorch models
            images_tensor = torch.from_numpy(images).float()
            outputs = model(images_tensor)
            _, preds = torch.max(outputs, 1)
            return preds.numpy()
        else:  # For traditional models
            images_flattened = images.reshape(images.shape[0], -1)  # Flatten for traditional models
            return model.predict(images_flattened)
# Get predictions
logistic_preds = get_predictions(logistic_model, test_images)
knn_preds = get_predictions(knn_model, test_images)
svm_preds = get_predictions(svm_model, test_images)
mlp_preds = get_predictions(mlp_model, test_images)
cnn_preds = get_predictions(cnn_model, test_images)
pretrained_preds = get_predictions(pretrained_model, test_images)

# Create a function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.show()

# Visualize confusion matrices for each model
plot_confusion_matrix(test_labels, logistic_preds, 'Logistic Regression')
plot_confusion_matrix(test_labels, knn_preds, 'K-Nearest Neighbors')
plot_confusion_matrix(test_labels, svm_preds, 'Support Vector Machine')
plot_confusion_matrix(test_labels, mlp_preds, 'Multilayer Perceptron')
plot_confusion_matrix(test_labels, cnn_preds, 'Convolutional Neural Network')
plot_confusion_matrix(test_labels, pretrained_preds, 'Pre-trained Model')

"""#### 4.4: Precision and Recall"""

report = classification_report(y_true, y_pred, target_names=[str(i) for i in range(10)])
print("Classification Report:")
print(report)

"""# 5. Save the models

## 5.1. Saving Traditional Models (e.g., Logistic Regression, KNN, SVM)
"""

import joblib

# Assuming `logistic_model`, `knn_model`, and `svm_model` are your trained models
joblib.dump(logistic_model, 'logistic_model.pkl')
joblib.dump(knn_model, 'knn_model.pkl')
joblib.dump(svm_model, 'svm_model.pkl')

"""#### 5.2. Saving Deep Learning Models (MLP, CNN)"""

# Assuming `mlp_model` and `cnn_model` are your trained models
torch.save(mlp_model.state_dict(), 'mlp_model.pth')
torch.save(cnn_model.state_dict(), 'cnn_model.pth')

# If using pre-trained models (like ResNet)
torch.save(pretrained_model.state_dict(), 'pretrained_model.pth')

"""## Challenges Faced During Exploration, Preprocessing, and Modeling"""

print("\nChallenges Faced:")
print("- Data Augmentation: Balancing the amount and type of transformations to prevent overfitting.")
print("- Noise Reduction: Finding the right filtering technique to improve image quality without losing important details.")
print("- Model Selection: Choosing between traditional and deep learning models based on data size and computational resources.")
print("- Hyperparameter Tuning: Optimizing model parameters for better performance.")

"""## Potential Improvements or Alternative Approaches"""

print("\nPotential Improvements:")
print("- Experiment with more advanced data augmentation techniques to improve model generalization.")
print("- Fine-tune hyperparameters using techniques like Grid Search or Random Search.")
print("- Explore other deep learning architectures like Transfer Learning with pre-trained models (e.g., ResNet, VGG).")
print("- Consider ensemble methods to combine the predictions from multiple models for improved accuracy.")

